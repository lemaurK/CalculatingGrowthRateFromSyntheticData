{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e141ece1",
   "metadata": {},
   "source": [
    "#  Extracting Statistical Information from AFM Scans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5152d240",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e3f5e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.signal import argrelextrema\n",
    "from findpeaks import findpeaks as findpeaks\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7577a5",
   "metadata": {},
   "source": [
    "## Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09ed4cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extpected input: Outputted cross sectional data matrix from Pygwy script,\n",
    "# pass the pixel dimension of your image as the value for n, defaulted to 512 representing a SQUARE 512x512 image.\n",
    "# Expected Output: Nested list, containing lists of a row-wise breakup of the inital matrix of size n by n.\n",
    "def SplitDataMatrixIntoRows(data, pixel_dimension_of_image=512):\n",
    "    nested_list_of_individual_cross_sections = list()\n",
    "    for i in range(0, len(data), pixel_dimension_of_image):\n",
    "        nested_list_of_individual_cross_sections.append(data[i:i + pixel_dimension_of_image])\n",
    "    return np.array(nested_list_of_individual_cross_sections)\n",
    "\n",
    "# Expected input: Output from SplitDataMatrixIntoRows(),\n",
    "# pass the physical length of your image in micrometers as \"length_in_micrometers\" and the same pixel dimension as \n",
    "# inputted into SplitDataMatrixIntoRows(), defaulted to length_in_micrometers=6 and pixel_dimension_of_image=512.\n",
    "# Expected Output: Nested list containing lists of x-axis locations of negative peaks (relative minima)\n",
    "def ExtractDistancesBetweenRelativeMinima_GrainDiameter(data, length_in_micrometers=6, pixel_dimension_of_image=512):\n",
    "    scaling_factor = length_in_micrometers/pixel_dimension_of_image\n",
    "    nested_list_of_xaxis_locations_of_relative_minima_for_each_cross_section = list()\n",
    "    for each_individual_cross_section in data:\n",
    "        xaxis_locations_of_relative_minima = argrelextrema(each_individual_cross_section, np.less)\n",
    "        xaxis_locations_of_relative_minima = xaxis_locations_of_relative_minima[0]\n",
    "        xaxis_locations_of_relative_minima = xaxis_locations_of_relative_minima*scaling_factor\n",
    "        nested_list_of_xaxis_locations_of_relative_minima_for_each_cross_section.append(xaxis_locations_of_relative_minima)\n",
    "    return nested_list_of_xaxis_locations_of_relative_minima_for_each_cross_section\n",
    "    \n",
    "# Expected Input: Output from ExtractDistancesBetweenRelativeMinima_GrainDiameter()  \n",
    "# Expected Output: Two floats: The average grain diameter for all grains in the image and the standard deviation.  \n",
    "def CalculateAverageAndStandardDeviation_GrainDiameter(data):\n",
    "    list_of_grain_diameters_for_all_cross_sections = list()\n",
    "    for each_list_of_relative_minima_locations in data:\n",
    "        list_of_individual_diameters = np.diff(each_list_of_relative_minima_locations)\n",
    "        list_of_grain_diameters_for_all_cross_sections.append(list_of_individual_diameters)\n",
    "    list_of_grain_diameters_for_all_cross_sections = np.concatenate(list_of_grain_diameters_for_all_cross_sections).ravel()\n",
    "    average_diameter = np.mean(list_of_grain_diameters_for_all_cross_sections)\n",
    "    standard_deviation_diameter = np.std(list_of_grain_diameters_for_all_cross_sections)\n",
    "    return average_diameter,standard_deviation_diameter\n",
    "\n",
    "# Expected Input: Output from SplitDataMatrixIntoRows().\n",
    "# Expected Output: Nested list containing heights of individual grains.\n",
    "def ExtractHeightOfRelativeMaxima_GrainHeight(data):\n",
    "    nested_list_of_grain_heights = list()\n",
    "    for each_individual_cross_section in data:\n",
    "        individual_heights = each_individual_cross_section[argrelextrema(each_individual_cross_section, np.greater)[0]]\n",
    "        nested_list_of_grain_heights.append(individual_heights)\n",
    "    return nested_list_of_grain_heights\n",
    "\n",
    "# Expected Input: Output from ExtractHeightOfRelativeMaxima_GrainHeight()\n",
    "# Expected Output: Two floats: The average grain height for all grains in the image and the standard devaiton.\n",
    "def CalculateAverageAndStandardDeviation_GrainHeight(data):\n",
    "    list_of_grain_heights_for_all_cross_sections = np.concatenate(data).ravel()\n",
    "    average_height = np.mean(list_of_grain_heights_for_all_cross_sections)\n",
    "    standard_deviation_height = np.std(list_of_grain_heights_for_all_cross_sections)\n",
    "    return average_height,standard_deviation_height\n",
    "\n",
    "# Expected Input: Output from ExtractDistancesBetweenRelativeMinima_GrainDiameter()\n",
    "# Expected Output: Four floats: The average maximum grain diameter from each cross section and the standard deviation.\n",
    "# The average minimum grain diameter from each cross section and the standard deviation.\n",
    "def CalculateAverageAndStandardDeviationOfMaximumAndMinimumGrainDiameter(data):\n",
    "    list_of_maximum_grain_diameter_from_each_cross_section = list()\n",
    "    list_of_minimum_grain_diameter_from_each_cross_section = list()\n",
    "    for each_individual_cross_section in data:\n",
    "        list_of_individual_diameters = np.diff(each_individual_cross_section)\n",
    "        list_of_maximum_grain_diameter_from_each_cross_section.append(max(list_of_individual_diameters))\n",
    "        list_of_minimum_grain_diameter_from_each_cross_section.append(min(list_of_individual_diameters))\n",
    "    average_maximum_grain_diameter = np.mean(list_of_maximum_grain_diameter_from_each_cross_section)\n",
    "    standard_deviation_of_maximum_grain_diameter = np.std(list_of_maximum_grain_diameter_from_each_cross_section)\n",
    "    average_minimum_grain_diameter = np.mean(list_of_minimum_grain_diameter_from_each_cross_section)\n",
    "    standard_deviation_of_minimum_grain_diameter = np.std(list_of_minimum_grain_diameter_from_each_cross_section)\n",
    "    return average_maximum_grain_diameter, standard_deviation_of_maximum_grain_diameter, average_minimum_grain_diameter, standard_deviation_of_minimum_grain_diameter\n",
    "\n",
    "# Expected Input: Output from ExtractHeightOfRelativeMaxima_GrainHeight()\n",
    "# Expected Output: Four floats: The average maximum grain height from each cross section and the standard deviation.\n",
    "# The average minimum grain height from each cross section and the standard deviation.\n",
    "def CalculateAverageAndStandardDeviationOfMaximumAndMinimumGrainHeight(data):\n",
    "    list_of_maximum_grain_heights_from_each_cross_section = list()\n",
    "    list_of_minimum_grain_heights_from_each_cross_section = list()\n",
    "    for each_individual_cross_section in data:\n",
    "        list_of_maximum_grain_heights_from_each_cross_section.append(max(each_individual_cross_section))\n",
    "        list_of_minimum_grain_heights_from_each_cross_section.append(min(each_individual_cross_section))\n",
    "    average_maximum_grain_height = np.mean(list_of_maximum_grain_heights_from_each_cross_section)\n",
    "    standard_deviation_of_maximum_grain_heights = np.std(list_of_maximum_grain_heights_from_each_cross_section)\n",
    "    average_minimum_grain_height = np.mean(list_of_minimum_grain_heights_from_each_cross_section)\n",
    "    standard_deviation_of_minimum_grain_heights = np.std(list_of_minimum_grain_heights_from_each_cross_section)\n",
    "    return average_maximum_grain_height, standard_deviation_of_maximum_grain_heights, average_minimum_grain_height, standard_deviation_of_minimum_grain_heights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826550eb",
   "metadata": {},
   "source": [
    "## Opening AFM Scan Data (2d arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b335d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = ['c11_5','c11_14','c11_31','c11_77','c11_130']\n",
    "\n",
    "for name in file_names:\n",
    "    with open(f'../../UR_la_plante/CaCO3 Gwyddion Python/gwyddion_scripts/gwy_data/{name}_full.txt') as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "        data = np.array(data[0])\n",
    "        exec(f'{name}_raw_data = data.astype(float)')\n",
    "\n",
    "for name in file_names:\n",
    "    exec(f'{name}_divided = SplitDataMatrixIntoRows({name}_raw_data)')\n",
    "    \n",
    "c11_scan_title = 'C11'\n",
    "c11_all_scans = {'c11_5_':c11_5_divided,\n",
    "                 'c11_14_':c11_14_divided,\n",
    "                 'c11_31_':c11_31_divided,\n",
    "                 'c11_77_':c11_77_divided,\n",
    "                 'c11_130_':c11_130_divided}\n",
    "c11_times = np.array([0.000001, 5, 14, 31, 77, 130])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9312f4fb",
   "metadata": {},
   "source": [
    "## Adding one more row to our data\n",
    "This must be done because for some reason the code to extract AFM data omitted the last row of our 2d array. Before adding a row the dimensions were 511x512 and after adding we get a square array of 512x512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7de4c26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('c11_5_', (511, 512))\n",
      "('c11_14_', (511, 512))\n",
      "('c11_31_', (511, 512))\n",
      "('c11_77_', (511, 512))\n",
      "('c11_130_', (511, 512))\n"
     ]
    }
   ],
   "source": [
    "# Before adding row\n",
    "for key, array in c11_all_scans.items():\n",
    "    print((key,array.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab5ba0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('c11_5_', (512, 512))\n",
      "('c11_14_', (512, 512))\n",
      "('c11_31_', (512, 512))\n",
      "('c11_77_', (512, 512))\n",
      "('c11_130_', (512, 512))\n"
     ]
    }
   ],
   "source": [
    "for key, array in c11_all_scans.items():\n",
    "    array = np.vstack([array, array[0]])\n",
    "    c11_all_scans[key] = array\n",
    "    print((key,array.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b8d819",
   "metadata": {},
   "source": [
    "## Initializing Data Generation Parameters\n",
    "Utilizing the \"Findpeaks\" Python package we will make a function that returns statistical information that will be passed in as parameters to out synthetic data generation functions necessary to generate representative data.\n",
    "\n",
    "**Module Docs for reference:** https://erdogant.github.io/findpeaks/pages/html/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf97f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_morphology_return_data_only(morphology_title, list_of_scans, times):\n",
    "    fp = findpeaks(method='topology', whitelist='peak')\n",
    "    scan_data = dict()\n",
    "    peak_y_cordinates = list()\n",
    "    data = {\"mu_n_grains\":list(),\n",
    "            \"sigma_n_grains\":list(),\n",
    "            \"mu_grain_width\":list(),\n",
    "            \"sigma_grain_width\":list(),\n",
    "            \"mu_grain_amp\":list(),\n",
    "            \"sigma_grain_amp\":list()}\n",
    "    \n",
    "    for index, scan in list_of_scans.items():\n",
    "        fit = fp.fit(scan)\n",
    "        list_of_peak_cordinates = fit['groups0']\n",
    "        for peak in list_of_peak_cordinates:\n",
    "            if peak[2] != 0.0:\n",
    "                peak_y_cordinates.append(peak[0][0])\n",
    "                    \n",
    "        list_of_crosssections_with_peaks = [scan[i] for i in list(set(peak_y_cordinates))]\n",
    "        \n",
    "        scan_mean_n_grains, scan_std_n_grains = len(list_of_peak_cordinates), 1\n",
    "        \n",
    "        scan_diams = ExtractDistancesBetweenRelativeMinima_GrainDiameter(list_of_crosssections_with_peaks)\n",
    "        scan_mean_diameter, scan_std_diameter = CalculateAverageAndStandardDeviation_GrainDiameter(scan_diams)\n",
    "        \n",
    "        scan_height = ExtractHeightOfRelativeMaxima_GrainHeight(list_of_crosssections_with_peaks)\n",
    "        scan_mean_height, scan_std_height = CalculateAverageAndStandardDeviation_GrainHeight(scan_height)\n",
    "        \n",
    "        collection = [scan_mean_n_grains, scan_std_n_grains,\n",
    "                      scan_mean_diameter, scan_std_diameter,\n",
    "                      scan_mean_height, scan_std_height]\n",
    "        \n",
    "        for entry, index in zip(collection, data.keys()): \n",
    "            data[index].append(entry)\n",
    "            \n",
    "    data['times'] = list(times)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc324cb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[findpeaks] >Finding peaks in 2d-array using topology method..\n",
      "[findpeaks] >Scaling image between [0-255] and to uint8\n",
      "[findpeaks] >Conversion to gray image.\n",
      "[findpeaks] >WARNING: Conversion to gray not possible.\n",
      "[findpeaks] >Denoising with [fastnl], window: [3].\n",
      "[findpeaks] >Detect peaks using topology method with limit at None.\n",
      "[findpeaks] >Fin.\n",
      "[findpeaks] >Finding peaks in 2d-array using topology method..\n",
      "[findpeaks] >Scaling image between [0-255] and to uint8\n",
      "[findpeaks] >Conversion to gray image.\n",
      "[findpeaks] >WARNING: Conversion to gray not possible.\n",
      "[findpeaks] >Denoising with [fastnl], window: [3].\n",
      "[findpeaks] >Detect peaks using topology method with limit at None.\n",
      "[findpeaks] >Fin.\n",
      "[findpeaks] >Finding peaks in 2d-array using topology method..\n",
      "[findpeaks] >Scaling image between [0-255] and to uint8\n",
      "[findpeaks] >Conversion to gray image.\n",
      "[findpeaks] >WARNING: Conversion to gray not possible.\n",
      "[findpeaks] >Denoising with [fastnl], window: [3].\n",
      "[findpeaks] >Detect peaks using topology method with limit at None.\n",
      "[findpeaks] >Fin.\n",
      "[findpeaks] >Finding peaks in 2d-array using topology method..\n",
      "[findpeaks] >Scaling image between [0-255] and to uint8\n",
      "[findpeaks] >Conversion to gray image.\n",
      "[findpeaks] >WARNING: Conversion to gray not possible.\n",
      "[findpeaks] >Denoising with [fastnl], window: [3].\n",
      "[findpeaks] >Detect peaks using topology method with limit at None.\n",
      "[findpeaks] >Fin.\n",
      "[findpeaks] >Finding peaks in 2d-array using topology method..\n",
      "[findpeaks] >Scaling image between [0-255] and to uint8\n",
      "[findpeaks] >Conversion to gray image.\n",
      "[findpeaks] >WARNING: Conversion to gray not possible.\n",
      "[findpeaks] >Denoising with [fastnl], window: [3].\n",
      "[findpeaks] >Detect peaks using topology method with limit at None.\n",
      "[findpeaks] >Fin.\n"
     ]
    }
   ],
   "source": [
    "# Running our data through the function.\n",
    "c11_original = process_morphology_return_data_only(c11_scan_title, c11_all_scans, c11_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5705fd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mu_n_grains': [1389, 881, 805, 688, 665],\n",
       " 'sigma_n_grains': [1, 1, 1, 1, 1],\n",
       " 'mu_grain_width': [0.18094496186159628,\n",
       "  0.21135960715893742,\n",
       "  0.2517335796659597,\n",
       "  0.325077236910093,\n",
       "  0.35365168205177006],\n",
       " 'sigma_grain_width': [0.09709316122674311,\n",
       "  0.10388979503856681,\n",
       "  0.12121153439079689,\n",
       "  0.1690740178063406,\n",
       "  0.194093005444742],\n",
       " 'mu_grain_amp': [44.66049567570916,\n",
       "  68.1558838765964,\n",
       "  59.00468987726128,\n",
       "  42.87113472897744,\n",
       "  38.16565087655431],\n",
       " 'sigma_grain_amp': [12.677142810314749,\n",
       "  11.900218095579584,\n",
       "  10.790749468725378,\n",
       "  8.321523550519661,\n",
       "  7.808176452173206],\n",
       " 'times': [1e-06, 5.0, 14.0, 31.0, 77.0, 130.0]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c11_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3053399",
   "metadata": {},
   "source": [
    "## Export Statistical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd492aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"statistical_data//c11_stats.csv\", \"w\") as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(c11_original.keys())\n",
    "    writer.writerows(zip(*c11_original.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ac0489c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu_n_grains</th>\n",
       "      <td>1389.000000</td>\n",
       "      <td>881.000000</td>\n",
       "      <td>805.000000</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>665.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma_n_grains</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_grain_width</th>\n",
       "      <td>0.180945</td>\n",
       "      <td>0.211360</td>\n",
       "      <td>0.251734</td>\n",
       "      <td>0.325077</td>\n",
       "      <td>0.353652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma_grain_width</th>\n",
       "      <td>0.097093</td>\n",
       "      <td>0.103890</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.169074</td>\n",
       "      <td>0.194093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_grain_amp</th>\n",
       "      <td>44.660496</td>\n",
       "      <td>68.155884</td>\n",
       "      <td>59.004690</td>\n",
       "      <td>42.871135</td>\n",
       "      <td>38.165651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma_grain_amp</th>\n",
       "      <td>12.677143</td>\n",
       "      <td>11.900218</td>\n",
       "      <td>10.790749</td>\n",
       "      <td>8.321524</td>\n",
       "      <td>7.808176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>times</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0           1           2           3           4\n",
       "mu_n_grains        1389.000000  881.000000  805.000000  688.000000  665.000000\n",
       "sigma_n_grains        1.000000    1.000000    1.000000    1.000000    1.000000\n",
       "mu_grain_width        0.180945    0.211360    0.251734    0.325077    0.353652\n",
       "sigma_grain_width     0.097093    0.103890    0.121212    0.169074    0.194093\n",
       "mu_grain_amp         44.660496   68.155884   59.004690   42.871135   38.165651\n",
       "sigma_grain_amp      12.677143   11.900218   10.790749    8.321524    7.808176\n",
       "times                 0.000001    5.000000   14.000000   31.000000   77.000000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('statistical_data/c11_stats.csv') as f:\n",
    "    df = pd.read_csv(f).T\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
